# The PAST Framework Field Guide

**From Random AI Experiments to Strategic Clarity**

---

## Introduction: The Strategic Clarity Problem



Most AI implementations fail before they start—not because of bad technology, but because of unclear strategy.

Walk into any organization today and you'll see the pattern: Marketing has their ChatGPT subscription. Sales discovered a different AI tool for lead scoring. Customer service is experimenting with chatbots. HR tests resume screening. The finance team bought yet another solution.

Each department discovered AI independently. Each chose different tools. Each measures different metrics. And none of them are talking to each other.

**The result?** Millions spent on tools that don't integrate. Teams working at cross-purposes. Data that can't flow between systems. Leadership that can't measure AI ROI because there's no coherent strategy to measure.

This isn't a technology problem. It's a strategy problem.

**The question everyone asks:** "How can we use AI?"

**The question they should ask:** "What business outcomes do we need to achieve?"

This shift—from technology-first to outcome-first thinking—is where the PAST Framework begins.

### What Makes PAST Different

The PAST Framework isn't just a corporate strategy checklist. It's a thinking framework that works at every level:

- **Organizational Strategy:** Enterprise AI implementation and governance
- **Team Workflows:** Department-specific process optimization
- **Individual Productivity:** Personal workflow enhancement
- **Prompt Engineering:** Creating effective AI interactions

The same four questions that guide billion-dollar AI implementations also create better prompts for daily tasks. This versatility is what makes PAST powerful—it's not just a strategy tool, it's a thinking framework that scales from individual tasks to organizational transformation.

### Who This Guide Is For

**Business professionals** starting with AI and wanting strategic clarity before random experimentation.

**Consultants** designing AI strategies for clients who need a proven framework.

**Team leaders** defining AI direction for their departments without getting lost in tool comparisons.

**Anyone writing prompts** who wants systematically better results.

**Solopreneurs** deciding which AI tools deserve investment of time and money.

### How to Use This Guide

Read the framework chapters (Purpose, Audience, Scope, Tone) to understand each element deeply.

Apply the worksheets to your specific situation—whether that's an organizational initiative, team workflow, or prompt design.

Use Chapter 5 (PAST for Prompt Engineering) as a quick-reference for daily AI interactions.

Return to Chapter 6 (Application Examples) when you need inspiration for specific contexts.

**What you won't find here:** Tool tutorials, feature comparisons, or technology recommendations. Those change monthly. What you will find: The strategic thinking that ensures any tool you choose actually delivers value.

---

## Chapter 1: Purpose — Why Are You Using AI?

### The Wrong Question vs. The Right Question

**The Wrong Question:** "How can we use AI?"

**The Right Question:** "What business outcomes do we need to achieve?"

Most AI implementations start with the technology and work backward to business value. This creates solutions looking for problems—the digital equivalent of buying a hammer and then wandering around looking for nails.

The purpose element of PAST forces outcome-first thinking. Before evaluating any tool, platform, or approach, you define what success actually looks like.

### Purpose at Different Levels

**At the Organizational Level**

Start with specific, measurable business outcomes:

**Revenue Growth:** "Increase sales conversion rates by 15% through better lead qualification"

**Cost Reduction:** "Reduce customer service response time by 50% while maintaining satisfaction scores"

**Risk Management:** "Improve fraud detection accuracy by 25% while reducing false positives"

**Operational Efficiency:** "Accelerate time-to-market for new products by 30%"

Notice what these have in common: Specific numbers. Clear metrics. Business outcomes, not technology adoption.

**At the Team Level**

Teams need purpose clarity that connects to broader organizational goals:

**Department-Specific:** "Marketing will increase qualified lead generation by 40% using AI-assisted content targeting"

**Workflow Improvement:** "The support team will reduce ticket resolution time from 4 hours to 1 hour through AI-powered initial triage"

**Collaboration Enhancement:** "Cross-functional teams will reduce meeting prep time by 60% using AI-generated briefing documents"

**At the Individual Level**

Personal productivity requires equally clear purpose:

**Time Recovery:** "Reduce time spent on weekly reporting from 3 hours to 30 minutes"

**Quality Improvement:** "Improve first-draft quality of client communications so editing time drops by 50%"

**Skill Development:** "Learn to use AI research tools effectively enough to handle 2x more client requests without increasing hours"

**At the Prompt Level**

Even individual prompts need purpose clarity:

**Bad Prompt (No Purpose):** "Write about AI implementation"

**Good Prompt (Clear Purpose):** "Create a 300-word executive summary explaining why 95% of AI pilots fail and what our organization can do differently. Emphasize ROI concerns for CFO review."

The first prompt could generate anything. The second has a clear purpose that shapes every word of the output.

### The Purpose Clarity Exercise

Complete this statement for each AI initiative:

> "Success means **[specific measurable outcome]** which will **[business impact]** by **[timeline]** as measured by **[metric]**."

**Organizational Example:**
"Success means reducing invoice processing time from 48 hours to 4 hours, which will free up 3 FTE for strategic work by Q2, as measured by average processing time and team capacity allocation."

**Team Example:**
"Success means eliminating 10 hours/week of manual data entry for the sales team, which will allow 2 additional client calls per rep per week by end of month, as measured by CRM activity logs and time tracking."

**Individual Example:**
"Success means cutting email drafting time from 90 minutes to 30 minutes daily, which will give me 5 extra hours per week for client work by next month, as measured by my time tracking app."

**Prompt Example:**
"Success means generating usable first-draft status updates 80% of the time, which will reduce my weekly reporting from 2 hours to 20 minutes, as measured by editing time required."

### Purpose Pitfalls to Avoid

**Vague Goals**
- "Improve efficiency" — Efficient at what? By how much? For whom?
- "Leverage AI" — For what outcome?
- "Stay competitive" — Against what benchmark?

**Technology-First Thinking**
- "Implement ChatGPT" — Why? What outcome?
- "Build a chatbot" — What problem does it solve?
- "Adopt AI tools" — Which ones? For what purpose?

**Competitor Copying**
- "Because everyone else is doing it" — Are they succeeding? At what?
- "We can't fall behind" — Behind at what, specifically?

**Solution Seeking Problems**
- "This AI tool looks cool" — Cool isn't a business outcome
- "Let's experiment" — Experiments need hypotheses

When you catch yourself falling into these patterns, return to the purpose clarity exercise. Specific, measurable, time-bound outcomes that matter to the business.

### The Metric Mandate



Before any AI project leaves the Purpose phase, it must pass through a hard gate.

Veljko Krunic puts it directly in *Succeeding with AI*: "The business metric must be defined for every single AI project. AI methods are, by their nature, quantitative. The inability to easily recognize business metrics by which an AI project should be measured raises a big red flag. If you can't quantify the business result you're hoping to achieve, you have to ask yourself and your stakeholders whether the project is worth doing."

**Metric Definition Gate:**

| Requirement | Must Have Answer |
|-------------|------------------|
| Named business metric | What specific number will change? |
| Current baseline | What is that number today? |
| Success threshold | What's the minimum improvement to justify investment? |
| Measurement timeline | When will we measure? |
| Kill criteria | What result means we stop? |

**If any field is blank or "TBD", the project is not ready for Audience analysis.** Return to Purpose.

The existing Purpose Clarity Exercise asks for outcomes and metrics, but doesn't enforce them as a gate. This addition creates a hard boundary—projects without quantified business results cannot proceed.

### Strategic Alignment Filter (1-Minute Check)

Before defining detailed Purpose, verify the initiative clearly connects to business strategy.

Dr. Janna Lipenkova frames this in *The Art of AI Product Development*: "Executives often want to know how the AI initiative supports the company's broader strategic goals. Does it open new markets? Improve customer retention? Reduce operational costs? Executives don't need to know the technical details of how machine learning works—they need to understand why it matters and how it will impact the bottom line."

**Strategic Alignment Check:**

Before defining detailed Purpose, verify the initiative clearly creates ONE of:

- [ ] **New revenue streams** (new markets, new products)
- [ ] **Better customer retention** (measurable CLV improvement)
- [ ] **Reduced costs** (quantified operational savings)

**If no box is checked → STOP.** Return to strategy alignment before investing time in detailed analysis.

### Purpose Quick Reference

Before any AI initiative, answer:

1. **What specific outcome are we trying to achieve?**
2. **How will we measure success?**
3. **By when do we need this outcome?**
4. **Why does this outcome matter to the business?**

If you can't answer all four clearly, you're not ready to evaluate tools. Return to purpose definition.

---

## Chapter 2: Audience — Who Does AI Serve?

### The User-Centered Imperative

AI implementations fail when they're designed for technology buyers instead of actual users.

The CIO who approves the purchase doesn't use the tool daily. The executive sponsor who loves the demo doesn't process invoices. The consultant who recommended the platform doesn't answer customer calls.

The audience element of PAST ensures you design for the people who will actually use AI tools—and the people who will receive AI-generated outputs.

### The Communication Failure Pattern



Most AI projects fail before implementation begins—in the pitch meeting.

Technical teams present AI initiatives in technical terms: model accuracy, training data quality, architecture decisions, integration complexity. Executives need business terms: revenue impact, competitive advantage, risk reduction, time to value.

Same project, completely different languages. The gap between those languages is where AI initiatives go to die.

**The Audience Translation Test:**

For each stakeholder group identified in Audience mapping, document:

| Stakeholder Group | What They Care About | Language to Use | Language to Avoid |
|-------------------|----------------------|-----------------|-------------------|
| Executive sponsors | ROI, competitive advantage | Business outcomes, revenue impact | Technical architecture, model accuracy |
| Daily users | Time savings, ease of use | Workflow improvement, friction reduction | API calls, integration complexity |
| IT/Security | Compliance, stability | Security controls, data governance | "Move fast and break things" |
| Finance | Predictable ROI | Implementation costs, returns timeline | Vague "efficiency gains" |
| The skeptics | Honest assessment | What we don't know yet, what could go wrong | Overselling, false confidence |

This extends Audience analysis beyond "who" to include "how to communicate with them"—preventing the pitch meeting failure pattern.

### The Reality Gap Assessment

Before finalizing Audience understanding, verify how work actually happens—not how it's documented.

Dr. Janna Lipenkova notes in *The Art of AI Product Development*: "How do people actually work versus how they're supposed to work? This gap often surprises leadership. Shadow processes exist because official processes don't match reality."

**Reality Gap Questions:**

- What tools do they actually use (vs. what's sanctioned)?
- Where do workarounds exist (and why)?
- What "unofficial" processes get real work done?
- Which formal processes are ignored (and why)?

**The Shadow AI Signal:** When users prefer unsanctioned AI tools, the problem isn't employee behavior—the problem is that official solutions don't match how work actually happens.

This ensures Audience analysis captures reality, not documented process. It prevents building AI solutions that technically fit process documentation but fail actual usage patterns.

### Mapping Your AI Stakeholders

**At the Organizational Level**

**Primary Users:** Who will interact with AI tools daily?
- Their current workflows and habits
- Their technical comfort level
- Their time constraints and pressures
- Their success metrics and incentives

**Secondary Users:** Who will receive AI outputs or insights?
- What decisions do they make with this information?
- What format do they prefer?
- How skeptical or trusting are they of AI?

**Decision Makers:** Who controls budgets and strategic direction?
- What proof do they need to continue investment?
- How do they define success?
- What failures have they seen before?

**Influencers:** Who shapes opinions about AI adoption?
- Who are the early adopters and enthusiasts?
- Who are the skeptics and resistors?
- Who do people trust for technology advice?

**IT/Security:** Who ensures compliance and integration?
- What are their non-negotiable requirements?
- What have they approved before?
- What concerns keep them up at night?

### Understanding Each Audience

For every audience you identify, document:

**Current Workflow:** How do they actually work today (not how the process document says they should work)?

**Pain Points:** What frustrates them most about current processes?

**Success Metrics:** How do they personally measure their effectiveness?

**Change Readiness:** How comfortable are they adopting new tools? What's their history?

**Required Training:** What support will they need to succeed?

### Audience at Different Levels

**At the Team Level**

When implementing AI for a specific team, answer:

- Who directly benefits from this workflow change?
- Who is impacted but not a primary user?
- What resistance should we expect and from whom?
- Who can champion this within the team?
- Whose buy-in is required for sustained adoption?

**At the Individual Level**

When optimizing your personal AI workflows:

- Who reads my AI-assisted outputs?
- What do they expect from my work?
- How will they judge quality?
- Are they aware I'm using AI? Does it matter?
- What's their tolerance for occasional AI errors?

**At the Prompt Level**

Every prompt has an implicit audience—the consumer of the output:

**Technical Teams:** Need precision, accuracy, and detail. Tolerate complexity. Want complete information.

**Executives:** Need conciseness, business impact framing, and clear recommendations. Intolerant of jargon or unnecessary detail.

**Customers:** Need empathy, clarity, and reliability. Zero tolerance for obvious AI artifacts or errors.

**Internal Colleagues:** Need actionable information at appropriate detail level for their role.

**Yourself (for reference):** Can use personal shorthand and abbreviated formats.

### Audience Alignment Questions

Before proceeding with any AI implementation, confirm:

- Does this solution make their job easier or harder?
- Will they see immediate value or only long-term benefits?
- How will this change their daily routines?
- What resistance should we expect and plan for?
- How will we measure adoption and satisfaction?
- Are we designing for the buyer or the user?

### The Empathy Mistake

Common audience failures to avoid:

**Designing for Yourself**
What works for your technical comfort level may frustrate less technical users. What you find intuitive may confuse others.

**Designing for Executives (Unless They're Users)**
The impressive demo that wows the boardroom may be impractical for the people who actually process orders, answer calls, or write reports.

**Designing for "Future State"**
The imagined efficient workflow ignores current habits, workarounds, and realities. People don't jump to future state—they evolve there through manageable steps.

**Ignoring the Resistors**
The skeptics often have valid concerns. Addressing them proactively builds broader adoption than hoping they'll come around.

### Audience Quick Reference

Before any AI initiative, map:

1. **Who will use this tool daily?** (Primary users)
2. **Who will receive outputs from this tool?** (Output consumers)
3. **Who must approve continued investment?** (Decision makers)
4. **Who can champion or undermine adoption?** (Influencers)
5. **Who has veto power over implementation?** (IT/Security/Compliance)

If primary users aren't enthusiastic, reconsider your approach. Technology imposed on reluctant users rarely succeeds.

---

## Chapter 3: Scope — Setting Realistic Boundaries

### Why Scope Matters More Than You Think

AI scope creep kills more projects than technical failures.

Every AI tool can do dozens of things. Every department can imagine hundreds of use cases. Every executive has their favorite "what if we could..." scenario.

Without clear scope, you'll try to solve everything and succeed at nothing.

**Research finding:** AI leaders pursue **half as many opportunities** but scale **more than twice as many successfully** as struggling peers. Tight scope isn't limitation—it's strategic focus that enables deep implementation.

### The Five Scope Dimensions



Every AI implementation needs clear boundaries around five dimensions:

**Functional Scope:** Which business processes are included?
- What specific workflows will AI augment?
- What adjacent processes are explicitly excluded?
- Where does AI assistance start and end?

**Data Scope:** What information will AI systems access?
- Which data sources are available?
- What data is off-limits?
- How is sensitive information protected?

**User Scope:** Who can use these tools and for what purposes?
- Which roles have access?
- What are they authorized to do?
- Who is explicitly excluded?

**Decision Scope:** What decisions can AI make autonomously versus requiring human approval?
- What can AI decide on its own?
- What requires human review?
- What is human-only?

**Timeline Scope:** What's the implementation roadmap and key milestones?
- What's Phase 1?
- What triggers expansion to Phase 2?
- What's the full vision (but not first priority)?

### The Capability Trap Connection

Remember: Organizations that succeed with AI pursue fewer opportunities more deeply.

The instinct to "AI all the things" leads to scattered experiments, none of which achieve critical mass for real value. The discipline to focus on one use case deeply—and scale it successfully—creates compound advantages over time.

**This means:** When defining scope, start narrow. Expand only after proving value in the initial scope.

### The Pilot Paradox



Veljko Krunic offers direct guidance in *Succeeding with AI*: "Don't start by chasing difficult projects that tie up all your resources and destroy you if they fail. Start instead with simple projects that have big business impact."

The instinct is to pursue impressive projects. The evidence supports pursuing strategically smart ones.

**The Pilot Selection Matrix:**

| Factor | High Priority | Low Priority |
|--------|---------------|--------------|
| Business impact | High | High (but complex) |
| Implementation complexity | Low | High |
| Data availability | Good | Poor |
| User enthusiasm | Strong | Resistant |
| Measurement clarity | Clear metrics | Vague outcomes |

**High Priority = Quick Win:** Simple implementation + clear impact = builds momentum for larger initiatives.

**Low Priority = Future Phase:** Complex implementation should wait until organizational confidence and capability are established.

This provides a concrete prioritization framework for Scope decisions, reducing the tendency to pursue "impressive" projects over strategically smart ones.

### Scope at Different Levels

**At the Organizational Level**

**Phase 1:** Single use case, limited user group, specific data set
- One department
- One workflow
- Clear boundaries
- Intensive support

**Phase 2:** Adjacent use cases, broader user group, expanded data access
- Same department, additional workflows
- OR: Same workflow, additional departments
- Proven patterns replicated

**Phase 3:** Cross-functional integration, organization-wide access, strategic decision support
- Multiple departments connected
- Data flowing between systems
- Enterprise-wide value creation

**At the Team Level**

Answer clearly:
- Which team processes get AI assistance?
- Which processes stay manual (for now)?
- What's the criteria for expanding scope?
- Who decides when scope changes?

**At the Individual Level**

Define for yourself:
- Which repetitive tasks get automated?
- How much complexity am I comfortable managing?
- Where does human judgment remain essential?
- What will I never delegate to AI?

**At the Prompt Level**

Constrain your prompts deliberately:

- **Length Limits:** "300-word summary" vs. "comprehensive analysis"
- **Information Sources:** "Use only the attached document" vs. "research broadly"
- **Output Format:** "Bullet points" vs. "narrative prose" vs. "structured table"
- **Detail Level:** "Executive overview" vs. "technical deep-dive"
- **Creative Freedom:** "Stick strictly to facts" vs. "provide interpretive analysis"

### The Scope Definition Template

Use this for any AI initiative:

**IN SCOPE:**

*Specific Processes:*
[List exact workflows that AI will augment]

*User Groups:*
[Define exactly who has access and for what]

*Data Sources:*
[Specify what information AI can use]

*Decision Authority:*
[Clarify what AI can decide vs. recommend]

**OUT OF SCOPE:**

*Excluded Processes:*
[What you're explicitly NOT automating yet]

*Restricted Users:*
[Who cannot access these AI tools]

*Protected Data:*
[Information AI cannot access]

*Human-Only Decisions:*
[What always requires human judgment]

### When to Expand Scope

Expand scope when:
- Current scope shows clear, measurable success
- User demand exists for expansion
- Resources are available to support growth
- Business case justifies additional investment
- Current implementation is stable

Do NOT expand scope because:
- Someone requested a new feature
- A competitor announced something
- A new AI capability became available
- You're bored with the current scope
- Leadership wants to see "more AI"

### Scope Quick Reference

Before any AI initiative, define:

1. **What specific workflows are included?** (Functional scope)
2. **What data can AI access?** (Data scope)
3. **Who can use this and for what?** (User scope)
4. **What can AI decide vs. recommend?** (Decision scope)
5. **What's Phase 1 vs. future phases?** (Timeline scope)

If you can't draw clear boundaries, your scope is too broad. Narrow until you can.

---

## Chapter 4: Tone — Aligning AI with Culture and Voice

### The Cultural Fit Imperative

AI implementations succeed when they feel natural within existing organizational culture. They fail when they fight against it.

An AI tool that requires formal documentation in a startup culture will be ignored. An AI assistant with casual output in a conservative law firm will be rejected. Tone isn't just about words—it's about cultural alignment.

But there's a deeper challenge: **AI-generated content can sound generic, indistinguishable from everyone else's AI output.** This "AI slop" problem undermines trust, damages brand differentiation, and signals lazy thinking to audiences.

### Cultural Alignment Considerations

**Risk Tolerance**

*Conservative Organizations:*
- Need extensive testing before rollout
- Require proof from similar organizations
- Want gradual adoption with multiple checkpoints
- Prefer established vendors over cutting-edge tools

*Innovative Cultures:*
- Embrace rapid experimentation
- Tolerate some failure in pursuit of learning
- Want to try new tools quickly
- Accept that some bets won't pay off

**Decision Making Style**

*Hierarchical Organizations:*
- AI must respect approval chains
- Outputs need to flow through proper channels
- Recommendations must be appropriate for each level
- Autonomy carefully bounded

*Collaborative Cultures:*
- AI should enhance team dynamics
- Outputs accessible to entire team
- Less concern about formal approval flows
- More experimentation at individual level

**Communication Patterns**

*Formal Organizations:*
- AI outputs need professional polish
- Integration with structured processes
- Consistent formatting requirements
- Documentation expectations

*Informal Cultures:*
- AI can be more casual
- Flexibility in format and style
- Speed valued over perfection
- Adaptation to individual preferences

### The AI Slop Problem



As AI-generated content proliferates, maintaining authentic voice becomes a competitive advantage. Generic AI output—what we call "AI slop"—undermines everything from sales emails to strategic documents.

**How to Detect AI Slop:**

**Generic Superlatives = Red Flag**

AI loves phrases like "impressive company" or "industry-leading solutions" because they're universally applicable. Genuine communication mentions specific things: "your Q3 product launch addressing mid-market banking compliance" instead of "your innovative solutions."

**The Specificity Test:** Could this exact text be sent to any business in your industry with minimal changes? If yes, it's AI slop.

**The Perfect Grammar Paradox**

AI produces flawless writing with zero original insights. Humans make occasional typos but demonstrate real understanding.

Don't optimize for grammatical perfection—optimize for authentic insight and industry-specific knowledge.

**The Flattery Sandwich Pattern**

AI slop follows this pattern: Compliment → Generic pitch → Compliment

*Example of Slop:* "Love what you're building. We help companies scale through proven strategies. Would love to discuss how we can support your impressive work."

*Example of Authentic:* "I noticed your Q3 implementation reduced customer support response time by 40% while maintaining NPS scores. Based on that systematic approach, our escalation workflow automation could eliminate the remaining manual handoffs in your tier-2 support process."

The difference: Specificity, domain knowledge, concrete detail.

### Maintaining Authentic Voice

**Specificity Over Polish**

Train yourself and your teams to value industry knowledge and specific details over perfect grammar and generic compliments.

*AI Slop:* "We help companies achieve remarkable efficiency improvements."

*Authentic:* "We helped FinServe Corp reduce regulatory reporting time from 80 hours to 12 hours using automated data validation."

**Challenge Generic Outputs**

When reviewing AI-generated content, ask: "Could this exact text be used by any company in our industry?"

If yes → Rewrite with specifics
If no → You've maintained differentiation

**Industry Knowledge Integration**

AI should enhance your domain expertise, not replace it.

*Formula:* Generic AI output + Your specific industry knowledge = Authentic voice

The AI provides structure and efficiency. You provide the insights that matter.

### Verification Protocols

Implement review processes specifically designed to catch AI slop:

- [ ] Does this mention specific outcomes or generic success metrics?
- [ ] Does this demonstrate industry knowledge or template insights?
- [ ] Could a competitor send essentially the same message?
- [ ] Does this reflect actual capabilities or universal claims?
- [ ] Would a knowledgeable reader recognize this as AI-generated?

If multiple checkboxes fail, rewrite with specificity and domain knowledge.

### Tone at Different Levels

**At the Organizational Level**

- How does your organization typically adopt new technologies?
- What internal communication style should AI tools reflect?
- How formal or informal should AI interactions feel?
- What level of autonomy do people expect from automated systems?
- How should AI handle errors or uncertain situations?

**At the Team Level**

- What's the team's communication culture?
- How much autonomy do team members expect?
- How does the team typically handle mistakes?
- What tone fits your team's working relationships?

**At the Individual Level**

- What's your personal work style?
- How do you typically communicate professionally?
- What tone serves your relationships with colleagues and clients?
- How much AI involvement should be visible in your outputs?

**At the Prompt Level**

Specify tone explicitly in prompts:

- **Professional vs. Casual:** "Write in formal business English" vs. "Keep it conversational"
- **Technical vs. Accessible:** "Use industry terminology" vs. "Explain for a non-technical audience"
- **Confident vs. Cautious:** "Present recommendations definitively" vs. "Acknowledge uncertainty"
- **Your Voice vs. Standard:** "Match my writing style in the attached samples" vs. "Use standard business communication"

### The Adoption Readiness Check

Dr. Janna Lipenkova emphasizes in *The Art of AI Product Development*: "AI's value isn't realized until it's successfully integrated into real-world workflows and embraced by users. To minimize adoption barriers, ensure the interface is easy to use and accessible for your team."

Before implementation, verify adoption readiness:

**Pre-Implementation Adoption Check:**

| Factor | Question | Red Flag |
|--------|----------|----------|
| Workflow Integration | How many steps change for users? | >3 step change |
| Interface Accessibility | Can non-technical staff use independently? | Requires IT support |
| Cost-Benefit Clarity | Is ROI quantified and believable? | "Efficiency gains" without numbers |
| Trust Factors | Are transparency, bias, privacy addressed? | No documentation |
| Training Load | How long until productive? | >1 day training |
| Fallback Plan | What happens when AI fails? | No manual backup |

**Scoring:** Any Red Flag = reconsider approach or reduce scope before proceeding.

This extends Tone beyond communication style to include adoption realism, ensuring implementations consider user readiness alongside cultural fit.

### Tone Quick Reference

Before any AI initiative, answer:

1. **How does your organization/team typically communicate?**
2. **What level of formality does your audience expect?**
3. **How do we prevent AI outputs from sounding generic?**
4. **What specific knowledge must come from humans, not AI?**
5. **How will we verify outputs maintain authentic voice?**

If your AI outputs could come from any organization in your industry, your tone strategy has failed.

---

## Chapter 5: PAST for Prompt Engineering



### Why PAST Works for Prompts

The same framework that guides organizational AI strategy creates effective prompts. This isn't coincidence—the underlying questions remain constant regardless of scale:

- **Purpose:** What are you trying to accomplish?
- **Audience:** Who receives or benefits from this?
- **Scope:** What's included and what's not?
- **Tone:** How should this feel and sound?

Whether you're defining enterprise AI strategy or crafting a single prompt, these four elements determine success or failure.

### Multi-Level Application Examples

**Example 1: Research Summary Prompt**



**Poor Prompt (No PAST):**
"Summarize this research about AI implementation"

This could generate anything—a 50-word overview, a 5,000-word analysis, technical deep-dive, casual summary. Without PAST clarity, you're gambling on what you'll receive.

**PAST Analysis:**

- **Purpose:** Create actionable insights for C-suite decision on Q4 AI investment (not comprehensive academic review)
- **Audience:** CFO and COO who need ROI justification, not technical details
- **Scope:** Focus on financial metrics and risk factors; exclude technical architecture discussions
- **Tone:** Executive brief format—concise bullets, emphasis on business impact, conservative risk framing

**Resulting Prompt:**
"Create a 300-word executive summary of this AI implementation research, focusing exclusively on ROI metrics, risk factors, and implementation costs. Target audience: CFO and COO evaluating Q4 investment. Use bullet points. Emphasize financial considerations over technical capabilities. Conservative tone appropriate for risk-averse leadership."

**Why This Works:** Every element of PAST constrains the output toward something specific and useful.

---

**Example 2: Customer Email Response**

**Poor Prompt (No PAST):**
"Write a response to this customer complaint"

**PAST Analysis:**

- **Purpose:** Acknowledge issue, explain resolution timeline, maintain customer relationship (not legal defense or deflection)
- **Audience:** Long-term customer (3+ years), technical background, expects detailed explanation
- **Scope:** Address specific API downtime complaint; don't discuss unrelated service features
- **Tone:** Professional empathy, technical accuracy, accountability (our brand voice: transparent, solution-focused)

**Resulting Prompt:**
"Draft a 200-word response to this API downtime complaint from a 3-year customer with technical background. Acknowledge the specific impact they described, explain root cause and resolution timeline with technical accuracy, and outline preventive measures. Tone: transparent accountability with solution focus. Avoid generic apology language—reference their specific use case."

---

**Example 3: Content Creation**

**Poor Prompt (No PAST):**
"Write a blog post about AI trends"

**PAST Analysis:**

- **Purpose:** Establish thought leadership while driving newsletter signups
- **Audience:** Business leaders considering AI adoption, non-technical, time-constrained
- **Scope:** 800-1000 words, focus on 3 trends with practical implications, include one contrarian take
- **Tone:** Conversational expert—accessible but authoritative, evidence-based not hype-driven

**Resulting Prompt:**
"Write an 800-word blog post about AI trends for 2026, targeting business leaders considering AI adoption. Cover 3 trends with practical implementation implications for each. Include one contrarian take that challenges conventional wisdom. Tone: conversational expert—accessible without being simplistic, evidence-based rather than hype-driven. End with a soft call-to-action for newsletter signup. Avoid jargon and generic AI enthusiasm."

---

**Example 4: Meeting Preparation**

**Poor Prompt (No PAST):**
"Prepare for my meeting tomorrow"

**PAST Analysis:**

- **Purpose:** Walk into meeting confident about client's business and ready with relevant questions
- **Audience:** Me (for preparation), then client (for discussion)
- **Scope:** Focus on their recent announcements, industry challenges, and potential fit with our services
- **Tone:** Informal notes for my reference, not client-facing

**Resulting Prompt:**
"Create a one-page briefing document for my meeting tomorrow with [Company]. Include: their recent news/announcements from the last 90 days, key industry challenges they likely face, 3 thoughtful questions I could ask that demonstrate understanding of their business, and potential alignment with our [specific service]. Format: bullet points for quick scanning. This is for my preparation only—casual tone is fine."

---

**Example 5: Data Analysis**

**Poor Prompt (No PAST):**
"Analyze this sales data"

**PAST Analysis:**

- **Purpose:** Identify why Q3 underperformed expectations for executive discussion
- **Audience:** Sales VP who needs to present findings to board
- **Scope:** Compare Q3 vs. Q2, focus on regional and product line variances, ignore metrics below 5% variance
- **Tone:** Analytical and direct, present problems clearly without softening

**Resulting Prompt:**
"Analyze the attached sales data comparing Q3 to Q2 performance. Identify the primary drivers of underperformance, focusing on regional and product line variances greater than 5%. This analysis is for the Sales VP to present to the board—be direct about problems without softening language. Format: Executive summary (3 sentences), then detailed findings in bullet points with specific numbers. Conclude with 2-3 questions the board is likely to ask."

### The PAST Prompt Template

Use this quick-reference for creating prompts:

**PURPOSE:** What specific outcome do I need?
- [ ] Information gathering
- [ ] Decision support
- [ ] Content creation
- [ ] Problem solving
- [ ] Communication drafting

**AUDIENCE:** Who will read/use this output?
- [ ] Internal team (what level?)
- [ ] External stakeholder (what relationship?)
- [ ] Just me (what's my goal?)
- [ ] Multiple audiences (prioritize which?)

**SCOPE:** What should be included/excluded?
- Length limit: _____ words/pages
- Information sources: _____
- Level of detail: _____
- What NOT to include: _____

**TONE:** How should this feel?
- [ ] Formal / Informal
- [ ] Technical / Accessible
- [ ] Concise / Comprehensive
- [ ] Cautious / Confident
- [ ] My voice / Standard business

**The Formula:**

> "[Action verb] a [length] [format] that [purpose] for [audience]. [Scope constraints]. [Tone specifications]."

### Why Prompt Engineering Matters

The difference between a vague prompt and a PAST-optimized prompt is the difference between gambling and strategy.

Vague prompts require extensive editing, multiple iterations, and often complete rewrites. PAST-optimized prompts deliver usable outputs on the first attempt most of the time.

The investment of 30 seconds thinking through PAST before writing a prompt saves minutes (sometimes hours) of rework.

**This is the real productivity gain from AI:** Not faster first drafts, but fewer iterations to final output.

---

## Chapter 6: PAST Application Examples Across Levels

### Organizational AI Strategy Example

**Company:** Mid-size consulting firm implementing AI for research and proposal development

**PAST Application:**

**Purpose:**
Reduce proposal development time by 40% while improving quality and win rates through better research synthesis and content personalization.

*Success Metrics:* Average proposal development time (current: 40 hours, target: 24 hours), proposal win rate (current: 28%, target: 35%), consultant satisfaction with tools

**Audience:**
- *Primary Users:* Senior consultants who write proposals
- *Secondary Users:* Partners who review and approve proposals
- *Decision Makers:* Practice leaders who control budgets
- *IT/Security:* Must ensure client data remains protected

*Key Insight:* Senior consultants are time-pressed and skeptical of "magic" tools. They need AI that clearly saves time without creating new problems.

**Scope:**
- *In Scope:* Research synthesis, competitive analysis, proposal section drafting
- *Out of Scope:* Client relationship management, final proposal approval, pricing decisions
- *Phase 1:* One practice area, 5 senior consultants, standard proposal templates
- *Phase 2:* All practice areas, proposal presentation support
- *Phase 3:* Integration with CRM and project management systems

*Key Constraint:* Start narrow. Prove value with 5 consultants before expanding.

**Tone:**
- Professional but approachable (matches consulting culture)
- Supports human expertise rather than replacing it
- Maintains high-quality standards expected by clients
- Respects confidentiality and client-specific customization needs
- **Avoids AI slop:** Proposals reference specific client challenges, not generic business compliments
- **Maintains differentiation:** Firm's domain expertise remains central; AI provides efficiency, not insights

**Result:** Clear implementation roadmap that respects organizational culture while delivering measurable business value and maintaining authentic client communication.

---

### Team Workflow Example

**Team:** Marketing team optimizing content creation workflow

**PAST Application:**

**Purpose:**
Increase content output by 50% while maintaining brand voice and quality standards. Enable team to shift from production to strategy.

*Success Metrics:* Content pieces published per month (current: 12, target: 18), time spent on production vs. strategy (current: 70/30, target: 50/50), brand consistency scores

**Audience:**
- *Primary Users:* Content creators (3 people)
- *Approver:* Brand manager
- *End Consumers:* Customers and prospects
- *Stakeholder:* CMO measuring marketing effectiveness

*Key Insight:* Content creators are creative professionals who fear AI will diminish their work. Position AI as freeing them for more creative tasks.

**Scope:**
- *In Scope:* Social media posts, blog post first drafts, email newsletter structure
- *Out of Scope:* Brand strategy, visual design, paid campaign copy (requires different approval)
- *Phase 1:* Social media posts only
- *Phase 2:* Blog posts
- *Phase 3:* Email newsletters

*Key Constraint:* Human review required before anything publishes. AI assists; humans decide.

**Tone:**
- Maintain existing brand voice (conversational, authentic, specific)
- AI provides structure; humans add personality and brand elements
- First drafts can be rough; final outputs must be polished
- No AI slop: Generic AI content fails brand review

**Result:** AI assists with first drafts and structure, humans add specificity and brand voice, 50% throughput increase achieved without quality decline.

---

### Individual Productivity Example

**Person:** Consultant who spends 10 hours/week on client status updates

**PAST Application:**

**Purpose:**
Reduce status update time from 10 hours to 2 hours while maintaining personalization that clients appreciate and expect.

*Success Metrics:* Time spent on updates (track weekly), client feedback on update quality, consultant stress levels around weekly reporting

**Audience:**
- *Output Consumers:* Individual clients, each with different communication preferences and technical levels
- *Secondary:* Account managers who read updates for oversight

*Key Insight:* Different clients want different things. Some want bullet points; others want narrative. Personalization matters.

**Scope:**
- *In Scope:* Weekly status updates only
- *Out of Scope:* Strategic recommendations, problem-solving communications, contract discussions
- *What AI Does:* First draft structure, data synthesis, standard language
- *What Human Does:* Personalization, tone calibration, relationship context

*Key Constraint:* Must maintain the personal touch that differentiates service.

**Tone:**
- Professional but warm
- Personalized to each client's preferences
- Progress-focused but transparent about challenges
- Never generic—must reference specific work and context

**Result:** Template-based system with AI assistance for structure and data synthesis. Human adds personalization. 80% time savings achieved. Client satisfaction maintained—one client even mentioned updates seemed "more thorough."

---

### Prompt Engineering Example

**Task:** Creating prompts for various business documents

**PAST Application for Each Document Type:**

**Executive Summary:**
- *Purpose:* Decision support for leadership
- *Audience:* C-suite (non-technical, time-constrained)
- *Scope:* 300 words max, financial focus, clear recommendation
- *Tone:* Formal, confident, direct

**Team Update:**
- *Purpose:* Status communication
- *Audience:* Project team (varied roles)
- *Scope:* Bullet points, key developments only
- *Tone:* Casual, transparent, action-oriented

**Customer Response:**
- *Purpose:* Maintain relationship and resolve issue
- *Audience:* Specific customer with specific concern
- *Scope:* Address their exact issue, nothing more
- *Tone:* Empathetic, solution-focused, accountable

**Internal Documentation:**
- *Purpose:* Knowledge capture for future reference
- *Audience:* Future team members who need context
- *Scope:* Complete but scannable
- *Tone:* Clear, organized, searchable

**Result:** Consistent high-quality outputs because prompts have strategic clarity. Less iteration, less rework, more useful first drafts.

---

## Conclusion: From Scattered Experiments to Strategic Advantage



### The Framework Versatility Advantage

You now have a thinking framework that works at every level:

**From organizational strategy to individual prompts.** The same four questions—Purpose, Audience, Scope, Tone—that guide enterprise AI implementation also create better prompts for daily tasks.

**From team workflows to personal productivity.** Whether you're designing an AI-assisted process for your department or optimizing your own email routine, PAST provides the structure for clarity.

You don't need different frameworks for different contexts. PAST scales because the underlying logic remains constant: Know why you're doing something, who it serves, what's included, and how it should feel.

### What Makes PAST Powerful

**Simple Enough to Remember:** Four questions. Purpose, Audience, Scope, Tone. You can apply this in 30 seconds.

**Comprehensive Enough to Prevent Major Mistakes:** These four elements catch the most common AI implementation failures before they start.

**Flexible Enough to Apply Anywhere:** Organizational strategy, team workflows, individual productivity, prompt engineering. Same framework, different scales.

**Systematic Enough to Ensure Consistency:** Once you internalize PAST thinking, you apply it automatically to every AI decision.

### Your Next Steps

**If You Need Strategic Clarity:**
1. Start with your biggest AI challenge
2. Apply the PAST Framework (Purpose, Audience, Scope, Tone)
3. Document your answers before choosing any tools
4. Use this clarity to guide all subsequent decisions

**If You Need Execution Methodology:**
You now have WHAT and WHY (from PAST). You need HOW and WHEN. See the **SHAPE Methodology Field Guide** for systematic execution from pilots to scale.

*PAST + SHAPE = Complete system from strategy to implementation*

**If You Need Personal Productivity:**
1. Apply PAST to your daily workflows—which tasks need strategic clarity?
2. Use PAST for every AI prompt you write
3. Watch your AI interactions become systematically better

### Remember

**Strategic clarity beats random experimentation.** The organizations succeeding with AI aren't the ones with the most tools—they're the ones with the clearest understanding of what they're trying to achieve.

**Framework thinking beats tool chasing.** New AI tools appear weekly. PAST thinking remains constant. Invest in the thinking, not just the tools.

**PAST works because the questions never change.** Whether you're defining organizational strategy or writing a single prompt:

- What's the **Purpose**?
- Who's the **Audience**?
- What's the **Scope**?
- What's the **Tone**?

Answer these four questions clearly, and you've already avoided the most common AI implementation failures.

---

## Worksheets & Templates

### PAST Framework Worksheet

*Use this for any AI initiative—organizational, team, individual, or prompt.*

---

**PURPOSE**

*What specific outcome are we trying to achieve?*

_______________________________________________________________

*Success looks like:*

_______________________________________________________________

*This will be measured by:*

_______________________________________________________________

*By when:*

_______________________________________________________________

---

**AUDIENCE**

*Primary users/consumers:*

_______________________________________________________________

*Secondary beneficiaries:*

_______________________________________________________________

*Decision makers:*

_______________________________________________________________

*What do they need most from this:*

_______________________________________________________________

---

**SCOPE**

*In scope:*

_______________________________________________________________

*Out of scope:*

_______________________________________________________________

*Phase 1 boundaries:*

_______________________________________________________________

*Expansion criteria:*

_______________________________________________________________

---

**TONE**

*Cultural fit considerations:*

_______________________________________________________________

*Communication style:*

_______________________________________________________________

*Formality level:*

_______________________________________________________________

*Voice characteristics:*

_______________________________________________________________

---

### PAST Prompt Engineering Template

*Quick reference for creating effective prompts.*

**PURPOSE:** What outcome do I need?
[ ] Information gathering
[ ] Decision support
[ ] Content creation
[ ] Problem solving
[ ] Communication drafting

**AUDIENCE:** Who reads/uses this?
[ ] Internal team (level: _______)
[ ] External stakeholder (relationship: _______)
[ ] Just me (goal: _______)
[ ] Multiple (priority: _______)

**SCOPE:**
- Length: _____ words
- Sources: _________________
- Detail level: ______________
- NOT including: ____________

**TONE:**
[ ] Formal / [ ] Informal
[ ] Technical / [ ] Accessible
[ ] Concise / [ ] Comprehensive
[ ] Cautious / [ ] Confident

**PROMPT FORMULA:**

> "[Action] a [length] [format] that [purpose] for [audience]. [Scope constraints]. [Tone specifications]."

---

### AI Slop Detection Checklist

*Quick audit for AI-generated content before sending.*

- [ ] Does this mention **specific outcomes** vs. generic metrics?
- [ ] Could this be sent to **any company in our industry** with minimal changes?
- [ ] Does this demonstrate **domain expertise** vs. template insights?
- [ ] Are there **generic superlatives** ("impressive," "industry-leading," "innovative")?
- [ ] Does this have **perfect grammar but zero original insights**?
- [ ] Would a knowledgeable reader **recognize this as AI-generated**?

**If 3+ checkboxes indicate problems:** Rewrite with specificity and domain knowledge.

**The Fix:** Add specific metrics, concrete examples, industry terminology, and context that only someone with real knowledge would include.

---

### PAST Enhancement Checklist

*Book-sourced gates and checks to strengthen each PAST phase.*

**Strategic Alignment Check (before Purpose):**
- [ ] Initiative maps to: Revenue / Retention / Cost Reduction
- [ ] Business case is one sentence (not a paragraph)
- [ ] Executive sponsor can explain why it matters

**Metric Mandate (during Purpose):**
- [ ] Named business metric defined
- [ ] Current baseline measured
- [ ] Success threshold set
- [ ] Measurement timeline established
- [ ] Kill criteria documented

**Communication Planning (during Audience):**
- [ ] Translation matrix completed for each stakeholder group
- [ ] Technical jargon removed from executive materials
- [ ] User workflow reality verified (not documented process)

**Pilot Prioritization (during Scope):**
- [ ] Pilot Selection Matrix completed
- [ ] Quick wins identified and prioritized
- [ ] Complex projects deferred to future phases

**Adoption Readiness (during Tone):**
- [ ] Pre-Implementation Adoption Check completed
- [ ] Red flags addressed or scope reduced
- [ ] Fallback plan documented

---

### Purpose Clarity Template

*Complete this statement before any AI initiative:*

> "Success means **[specific measurable outcome]** which will **[business impact]** by **[timeline]** as measured by **[metric]**."

**Example:**
"Success means reducing proposal development time from 40 hours to 24 hours, which will enable consultants to handle 50% more client engagements by Q2, as measured by time tracking and engagement capacity."

---

## About the Author

Jim Christian helps organizations implement AI systematically through observation-based frameworks, not ideological debates about AI's societal impact.

With over 30 years in technology consulting and digital transformation, Jim has worked across industries from Fortune 500 financial institutions to regional SMEs, consistently focusing on one question: What actually works?

This curiosity-driven approach led to the development of the PAST and SHAPE frameworks featured in this guide—systematic methodologies now used by organizations worldwide to turn scattered AI experiments into sustainable competitive advantages.

Jim is the author of Signal Over Noise, a weekly newsletter serving professionals who need practical AI implementation guidance without the hype.

Based in Valencia, Spain, Jim works with clients globally, bringing three decades of implementation experience to organizations ready to move from AI experimentation to systematic advantage.

**Connect:** [Newsletter](https://signalovernoise.at) | [Website](https://jimchristian.net)

---

*For ongoing AI implementation insights and strategic guidance, subscribe to Signal Over Noise at signalovernoise.at*

*For systematic execution methodology, see The SHAPE Methodology Field Guide.*

*PAST + SHAPE = Complete system from strategy to implementation.*

---

## License

This work is licensed under a [Creative Commons Attribution-ShareAlike 4.0 International License](https://creativecommons.org/licenses/by-sa/4.0/).
